{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85624dfb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img align=\"left\" src=\"img/ECE364-logo.png\" width=\"300px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "# Lecture 2 - Linear Algebra I\n",
    "## ECE364 - Programming Methods for Machine Learning\n",
    "### Nickvash Kani \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### Slides based off prior lectures by Alex Schwing, Aigou Han, Farzas Kamalabadi, Corey Snyder. All mistakes are my own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a33a04d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Things we'll cover in today's lecture:\n",
    "\n",
    "- Revisit computation graphs\n",
    "- Assorted matrix operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c852391",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb5354",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Motivation: \n",
    "\n",
    "What are we hoping to learn? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c6c267",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's think of a convolutional neural network we see in literature. We input a image and get out the probability that the image is one of several classifications:\n",
    "\n",
    "<img align=\"center\" src=\"img/Basic_network_-1.png\" width=\"1200px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f52fb30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ofcourse, there is a ground truth that we need to consider. Model seems off? \n",
    "\n",
    "<img align=\"center\" src=\"img/Basic_network_0.png\" width=\"1200px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734c4d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The output layer needs to be corrected according to the model errors. \n",
    "\n",
    "<img align=\"center\" src=\"img/Basic_network_1.png\" width=\"1200px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fb4efa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which means the prior layer should also be adjusted according to the error in the output layer.\n",
    "\n",
    "<img align=\"center\" src=\"img/Basic_network_2.png\" width=\"1200px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbce40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And we adjust back accordingly.\n",
    "\n",
    "<img align=\"center\" src=\"img/Basic_network_3.png\" width=\"1200px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6075f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How computation graphs work (in PyTorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a897ee86",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's reconsider the simple network we talked about in last lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814016a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember we had a multi-function network: \n",
    "\n",
    "<img align=\"center\" src=\"img/comp_graph_example_1d.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85f27ca",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Where the functions above are defined as follows: \n",
    "\n",
    "$g(x) = x^2+1$, $h(g) = \\log(g)$, $k(h) = \\sin(h)$. Thus, $f(x) = k(h(g(x)))$\n",
    "\n",
    "The resulting function is: $$f(x) = \\sin(\\log(((x)^2+1)))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd13a1e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\frac{df}{dx} = \\left.\\frac{dk}{dh}\\right|_{h(g)} \\cdot \\left.\\frac{dh}{dg}\\right|_{g(x)} \\cdot \\left.\\frac{dg}{dx}\\right|_x$$\n",
    "\n",
    "$$\\frac{df}{dx} = \\cos(h(g)) \\cdot \\frac{1}{g(x)} \\cdot 2x = \\cos(\\log(x^2+1)) \\cdot \\frac{1}{x^2+1} \\cdot 2x= \\frac{2x\\cos(\\log(x^2+1))}{x^2+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdeaa76a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Function to compute y = sin(log(x^2 + 1))\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfunc\u001b[39m(x):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Function to compute y = sin(log(x^2 + 1))\n",
    "def func(x):\n",
    "    g = x**2 + 1\n",
    "    h = torch.log(g)\n",
    "    k = torch.sin(h)\n",
    "    return k\n",
    "    #return torch.sin(torch.log(x**2 + 1))\n",
    "\n",
    "# Set up the initial guess for x (starting point for optimization)\n",
    "x = torch.tensor([1.0], requires_grad=True)  # x is a scalar, requires gradients\n",
    "\n",
    "# Use an optimizer to minimize the difference between y and 0\n",
    "optimizer = torch.optim.Adam([x], lr=0.1)\n",
    "\n",
    "# Number of iterations for optimization\n",
    "num_iterations = 20\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Zero the gradients from the previous step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the function value\n",
    "    y = func(x)\n",
    "    \n",
    "    print(x)\n",
    "    # Compute the loss, we want to find x when y = 0\n",
    "    loss = y**2  # This is the squared error between y and 0\n",
    "\n",
    "    #Calculate manual gradient of x\n",
    "    manual = 2*x*torch.cos(torch.log(x**2+1))/(x**2+1)\n",
    "    \n",
    "    # Perform backward propagation to compute the gradients\n",
    "    y.backward()\n",
    "\n",
    "    # Print the current value of x and loss at every 100 iterations\n",
    "    if i % 1 == 0:\n",
    "        print(f\"Iteration {i}, x: {x.item():.4f}, y: {y.item():.6f}, Auto Gradient of x: {x.grad.item():.6f}, Manual Gradient of x: {manual.item():.6f}\")\n",
    "\n",
    "    # Update the value of x using the optimizer\n",
    "    optimizer.step()        \n",
    "        \n",
    "# Final value of x where y should be approximately 0\n",
    "print(f\"Final value of x: {x.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b5eb6d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So the major question is how does the computer keep track of the gradient calculation? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f481544",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Through computation graphs! But how does PyTorch create computation graphs? Let's go through the prior example:\n",
    "\n",
    "Good references: \n",
    "1. https://pytorch.org/blog/computational-graphs-constructed-in-pytorch/\n",
    "2. https://pytorch.org/blog/overview-of-pytorch-autograd-engine/\n",
    "3. http://blog.ezyang.com/2019/05/pytorch-internals/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e919877",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Starting off with ```x``` we need to tell PyTorch to create a new computation graph. We do this by turning on the ```requires_grad``` option of the tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca73dfc2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.], requires_grad=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf43cad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"center\" src=\"img/comp_graph_0.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "Under the hood, whats happening is that the ```requires_grad``` option allocates a ```AutogradMeta``` object that stores the graph information. The graph stores derivative functions and values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe48d54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we define the next step in our composite function ```g(x)```.  The resulting tensor has a pointer to a field called ```grad_fn``` which is function that will be used to calculate the backward gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24fbc528",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], grad_fn=<PowBackward0>)\n",
      "tensor([2.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g1 = x**2\n",
    "print(g1)\n",
    "g = g1+1\n",
    "print(g) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b68bb9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"center\" src=\"img/comp_graph_1.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "Notice that even though ```g(x)``` is defined in one line, it contains two backward functions. A backward function is inserted into the graph whenever a new operation is performed on a tensor with a gradient activation flag. \n",
    "\n",
    "PyTorch has a file called ```tools/autograd/derivatives/yaml``` that stores the derivatives of many (most?) functions you will see (you can manually define a custom function/derivative yourself as well!)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b20382",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Next we want to define the ```log``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3223e7d6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6931], grad_fn=<LogBackward0>)\n"
     ]
    }
   ],
   "source": [
    "h = torch.log(g)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d02bdf1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"center\" src=\"img/comp_graph_2.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e01e5ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally we define the ```sin``` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "269d070b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6390], grad_fn=<SinBackward0>)\n"
     ]
    }
   ],
   "source": [
    "k = torch.sin(h)\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c49e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img align=\"center\" src=\"img/comp_graph_3.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50914c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As mentioned earlier we can view the network using the ```tensorboard``` utility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b2f65ffd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/experiment_2')\n",
    "class OurNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OurNet,self).__init__()\n",
    "    def forward(self,x):\n",
    "        g1 = x**2\n",
    "        g = g1 + 1\n",
    "        h = torch.log(g)\n",
    "        k = torch.sin(h)\n",
    "        return k\n",
    "graph = OurNet()\n",
    "writer.add_graph(graph,torch.randn((1)))\n",
    "writer.close()\n",
    "\n",
    "### use tensorboard --logdir=runs command to view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2087fa3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Live tensorboard demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc84ebc8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So now that we understand computation graphs, how do we get from: \n",
    "\n",
    "<img align=\"center\" src=\"img/comp_graph_example_1d.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n",
    "\n",
    "to:\n",
    "\n",
    "<img align=\"center\" src=\"img/Basic_network_-1.png\" width=\"600px\" style=\"padding:30px;border:thin solid white;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88805084",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Algebra for Machine Learning\n",
    "Linear algebra is a core mathematical concept in machine learning, especially deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9fd25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Scaler, vector, matrix and tensor (Review)\n",
    "\n",
    "* Scaler - A scalar is a single number \n",
    "* Vector - 1-D list of numbers\n",
    "* Matrix - 2-D list of numbers\n",
    "* Tensor - N-D list of numbers\n",
    "    <div> <img src=\"attachment:1_PDC6NHVmFXFZqxFf4YRSDg.png\" width=\"550\"/> </div>\n",
    "* We refer to anything which has three or more dimensions as a tensor rather than a matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91871868",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix Transpose\n",
    "* The transpose of a matrix is found by switching its rows with its columns. The transpose of the matrix can be thought of as a mirror image across the main diagonal.\n",
    "\n",
    "    $A=\\left[\\begin{matrix}a_{11} & a_{12} & \\ldots & a_{1n}\\\\\n",
    "a_{21} & a_{22} & \\vdots & a_{2n}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{m1} & a_{m2} & \\ldots & a_{mn}\n",
    "\\end{matrix}\\right]$;<Br>\n",
    "__Transpose matrix:__ $A^{T}=\\left[\\begin{matrix}a_{11} & a_{21} & \\ldots & a_{m1}\\\\\n",
    "a_{12} & a_{22} & \\vdots & a_{m2}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "a_{1n} & a_{2n} & \\ldots & a_{mn}\n",
    "\\end{matrix}\\right]$ <Br>\n",
    "    \n",
    "    \n",
    "* In Pytorch you can transpose a matrix in two ways: **torch.t()** or **torch.transpose()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04ef340",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose matrix of a is tensor([[1, 1],\n",
      "        [2, 2],\n",
      "        [3, 3]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([\n",
    "    [1, 2, 3],\n",
    "    [1, 2, 3]\n",
    "])\n",
    "transpose_a = torch.t(a)\n",
    "print(\"Transpose matrix of a is\",transpose_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "933676b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose matrix of a is tensor([[1, 1],\n",
      "        [2, 2],\n",
      "        [3, 3]])\n"
     ]
    }
   ],
   "source": [
    "transpose_a = torch.transpose(a,0,1)\n",
    "print(\"Transpose matrix of a is\",transpose_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f42f214",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Inner (dot) product of two vectors\n",
    "* The inner product takes two **vectors of equal size** and returns a single number (scalar). This is calculated by multiplying the corresponding elements in each vector and adding up all of those products.\n",
    "* For example: $\\left[\\begin{array}{c}\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\n",
    "\\end{array}\\right]\\cdot\\left[\\begin{array}{c}\n",
    "4\\\\\n",
    "5\\\\\n",
    "6\n",
    "\\end{array}\\right]=1\\times4+2\\times5+3\\times6=32$\n",
    "* In Pytorch we use **torch.dot()** to calculate innder product of the **vectors that are the same size.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "355ea06d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([4, 5, 6])\n",
      "Inner product of a and b is tensor(32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([4, 5, 6])\n",
    "\n",
    "ab= torch.dot(a, b)\n",
    "print (a)\n",
    "print (b)\n",
    "print (\"Inner product of a and b is\",ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c48f5b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* This operation is also commutative, in which $\\boldsymbol{a}\\cdot\\boldsymbol{b}=\\boldsymbol{b}\\cdot\\boldsymbol{a}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36c8a274",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inner product of b and a is tensor(32)\n"
     ]
    }
   ],
   "source": [
    "ba= torch.dot(b, a)\n",
    "print (\"Inner product of b and a is\",ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "23ab3e2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected tensor [2] and src [3] to have the same number of elements, but got 2 and 3 elements respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m      3\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m ac\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdot(a, c)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInner product of a and b is\u001b[39m\u001b[38;5;124m\"\u001b[39m,ac)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [2] and src [3] to have the same number of elements, but got 2 and 3 elements respectively"
     ]
    }
   ],
   "source": [
    "# what if now the vectors are not the same size\n",
    "a = torch.tensor([1, 2])\n",
    "c = torch.tensor([3, 2, 1])\n",
    "\n",
    "ac= torch.dot(a, c)\n",
    "print (\"Inner product of a and b is\",ac)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a30b21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix-Vector dot product\n",
    "\n",
    "* Multiplication between a matrix $A$ and vector $x$ is given as:\n",
    "\n",
    "    $Ax = \\left[ \\begin{matrix} a_{11} & a_{12} & \\ldots & a_{1n} \\\\ a_{21} & a_{22} & \\vdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\ldots & a_{mn} \\end{matrix} \\right] \\left[ \\begin{matrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{matrix} \\right] = \\left[ \\begin{matrix} a_{11} x_{1} + a_{12} x_{2} + \\ldots + a_{1n} x_{n} \\\\ a_{21} x_{1} + a_{22} x_{2} + \\ldots + a_{2n} x_{n} \\\\ \\ldots \\\\ a_{m1} x_{1} + a_{m2} x_{2} + \\ldots + a_{mn} x_{n} \\end{matrix} \\right]$\n",
    "* This method computes matrix dot product with a vector by taking an **𝑚×𝑛** 2D Tensor (matrix) and an **𝑛** 1D Tensor (vector). The result is a **m** 1D Tensor (vector).\n",
    "* In Pytorch we use **torch.Mv()** to calculate dot product of the **a 2D matrix and a vector.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7a028b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1864, -0.9706, -0.2550],\n",
      "        [ 1.4466, -1.0246, -0.4683]])\n",
      "tensor([ 0.0371,  1.0178, -0.9256])\n",
      "Matrix-vector dot product is tensor([-0.7588, -0.5557])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a  =  torch.randn(2, 3) \n",
    "b  =  torch.randn(3)\n",
    "print(a)\n",
    "print(b)\n",
    "c = torch.mv(a,b)\n",
    "print(\"Matrix-vector dot product is\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d58e68a9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication is tensor([-4.4887, -1.6265])\n"
     ]
    }
   ],
   "source": [
    "d = a@b\n",
    "print(\"Matrix multiplication is\", d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1486a91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wait a minute, if ```torch.mm``` and ```torch.mv``` do the same thing, why does ```torch.mv``` even exist? `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd07726",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47ebb7e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken by torch.mv: 0.009708 seconds\n",
      "Time taken by torch.mm: 0.012335 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "\n",
    "# Define the size of the matrix and vector\n",
    "m, n = 1000, 1000\n",
    "\n",
    "# Create a random matrix A (m x n) and a random vector v (n,)\n",
    "A = torch.rand(m, n)\n",
    "v = torch.rand(n)\n",
    "\n",
    "# Time `torch.mv` (matrix-vector multiplication)\n",
    "start_time = time.time()\n",
    "result_mv = torch.mv(A, v)\n",
    "mv_time = time.time() - start_time\n",
    "\n",
    "# Time `torch.mm` (matrix-matrix multiplication)\n",
    "start_time = time.time()\n",
    "# We need to reshape v to be a matrix with shape (n, 1) for mm\n",
    "result_mm = torch.mm(A, v.view(-1, 1))\n",
    "mm_time = time.time() - start_time\n",
    "\n",
    "# Print the results\n",
    "print(f\"Time taken by torch.mv: {mv_time:.6f} seconds\")\n",
    "print(f\"Time taken by torch.mm: {mm_time:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a152c7c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hadamard Product (Element Wise Multiplication)\n",
    "* Hadamard product of two vectors/matrices is very similar to matrix addition, elements corresponding to same row and columns of given vectors/matrices are multiplied together to form a new vector/matrix.\n",
    "* The order of matrices/vectors to be multiplied should be same and the resulting matrix will also be of same order.\n",
    "* Example: $\\left[\\begin{array}{cc}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{array}\\right]\\circ\\left[\\begin{array}{cc}\n",
    "5 & 6\\\\\n",
    "7 & 8\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "\\left(1\\times5\\right) & \\left(2\\times6\\right)\\\\\n",
    "\\left(3\\times7\\right) & \\left(4\\times8\\right)\n",
    "\\end{array}\\right]=\\left[\\begin{array}{cc}\n",
    "5 & 12\\\\\n",
    "21 & 32\n",
    "\\end{array}\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "986c9e4e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "tensor([[5, 6],\n",
      "        [7, 8]])\n",
      "Hadamard product of a and b is tensor([[ 5, 12],\n",
      "        [21, 32]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1, 2],[3, 4]])\n",
    "b = torch.tensor([[5, 6],[7, 8]])\n",
    "\n",
    "ab = a*b\n",
    "print (a)\n",
    "print (b)\n",
    "print (\"Hadamard product of a and b is\",ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05e21dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix dot product\n",
    "* **Dot product of two matrices** requires the matrices to have certain sizes. The number of columns of the first matrix must be equal to the number of rows of the second matrix. Each row of the first matrix will be transposed and multiplied against each column in the second matrix. This is basically a vector multiplication where each row in the first matrix is transposed to make sure it has the same dimension as each column in the second matrix.\n",
    "* For example:\n",
    "$\\left[\\begin{array}{cc}\n",
    "1 & 2\\\\\n",
    "3 & 4\\\\\n",
    "5 & 6\n",
    "\\end{array}\\right]\\cdot\\left[\\begin{array}{ccc}\n",
    "{\\color{brown}1} & {\\color{brown}2} & {\\color{brown}3}\\\\\n",
    "{\\color{brown}4} & {\\color{brown}5} & {\\color{brown}6}\n",
    "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
    "\\left(1\\times{\\color{brown}1}+2\\times{\\color{brown}4}\\right) & \\left(1\\times{\\color{brown}2}+2\\times{\\color{brown}5}\\right) & \\left(1\\times{\\color{brown}3}+2\\times{\\color{brown}6}\\right)\\\\\n",
    "\\left(3\\times{\\color{brown}1}+4\\times{\\color{brown}4}\\right) & \\left(3\\times{\\color{brown}2}+4\\times{\\color{brown}5}\\right) & \\left(3\\times{\\color{brown}3}+4\\times{\\color{brown}6}\\right)\\\\\n",
    "\\left(5\\times{\\color{brown}1}+6\\times{\\color{brown}4}\\right) & \\left(5\\times{\\color{brown}2}+6\\times{\\color{brown}5}\\right) & \\left(5\\times{\\color{brown}3}+6\\times{\\color{brown}6}\\right)\n",
    "\\end{array}\\right]=\\left[\\begin{array}{ccc}\n",
    "9 & 12 & 15\\\\\n",
    "19 & 26 & 33\\\\\n",
    "29 & 40 & 51\n",
    "\\end{array}\\right]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f0c4d5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* In Pytorch we use **torch.mm()** to calculate Dot product of **two 2-dimentional matrices**. This method computes matrix dot product by taking an  𝑚×𝑛  Tensor and an  𝑛×𝑝  Tensor. It can deal with only two-dimensional matrices and not with single-dimensional ones. This function does not support broadcasting. Broadcasting is nothing but the way the tensors are treated when their shapes are different. The smaller Tensor is broadcasted to suit the shape of the wider or larger Tensor for operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b18de43c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "Dot product of a and b is tensor([[22, 28],\n",
      "        [49, 64]])\n"
     ]
    }
   ],
   "source": [
    "# Example of matrix dot product\n",
    "a = torch.arange(1, 7).view(2, 3)\n",
    "b = torch.arange(1, 7).view(3, 2)\n",
    "print(a)\n",
    "print(b)\n",
    "adotb=torch.mm(a, b)\n",
    "print (\"Dot product of a and b is\",adotb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbac3103",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "Dot product of a and b is\n",
      "tensor([[ 9, 12, 15],\n",
      "        [19, 26, 33],\n",
      "        [29, 40, 51]])\n"
     ]
    }
   ],
   "source": [
    "# Example of matrix dot product\n",
    "a = torch.arange(1, 7).view(3, 2)\n",
    "b = torch.arange(1, 7).view(2, 3)\n",
    "print(a)\n",
    "print(b)\n",
    "adotb=torch.mm(a, b)\n",
    "print (\"Dot product of a and b is\")\n",
    "print (adotb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f295649",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Unlike inner product, matrix multiplication is not commutative, in which $\\boldsymbol{a}\\cdot\\boldsymbol{b}\\neq\\boldsymbol{b}\\cdot\\boldsymbol{a}$ <Br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9c75046c",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product of b and a is tensor([[ 9, 12, 15],\n",
      "        [19, 26, 33],\n",
      "        [29, 40, 51]])\n"
     ]
    }
   ],
   "source": [
    "# Matrix multiplication is not commutative,\n",
    "bdota=torch.mm(b, a)\n",
    "print (\"Dot product of b and a is\",bdota)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8181e45f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Example below shows that it is important to make sure the rows of the first matrix have the same number of entries as the columns of the second matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e9609d7f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# the dimensions of the matrices is important for matrix multiplication\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m adota\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmm(a, a)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDot product of a and a is\u001b[39m\u001b[38;5;124m\"\u001b[39m,adota)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# the dimensions of the matrices is important for matrix multiplication\n",
    "adota=torch.mm(a, a)\n",
    "print (\"Dot product of a and a is\",adota)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e54e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Matrix product of two tensors (general form)\n",
    "* In Pytorch we use **torch.matmul(Tensor_1, Tensor_2, out=None)** to compute the multiplication of two vector matrices (single-dimensional matrices), 2D matrices and mixed ones also. This method also supports broadcasting and batch operations. Depending upon the input matrices dimensions, the operation to be done is decided.\n",
    "* The table below lists the various possible dimensions of the arguments and the operations based on it.\n",
    "| Argument 1 | Argument 2 | Action taken |\n",
    "| :- | :- | :-: |\n",
    "| 1-dimensional | 1-dimensional | The scalar product is calculated\n",
    "| 2-dimensional | 2-dimensional | General matrix multiplication is done\n",
    "| 1-dimensional | 2-dimensional | The tensor 1 is pretended with a ‘1’ to match dimension of tensor 2\n",
    "| 2-dimensional | 1-dimensional | Matrix-vector product is calculated\n",
    "| 1/N-dimensional (N>2)\t | 1/N-dimensional (N>2)\t | Batched matrix multiplication is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fb51d935",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single dimensional tensors : tensor(36)\n"
     ]
    }
   ],
   "source": [
    "# both arguments 1D\n",
    "vec_1 = torch.tensor([3, 6, 2])\n",
    "vec_2 = torch.tensor([4, 1, 9])\n",
    "  \n",
    "print(\"Single dimensional tensors :\", torch.matmul(vec_1, vec_2))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "482f4f5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3x3 dimensional tensors :\n",
      " tensor([[10, 28, 28],\n",
      "        [27, 73, 62],\n",
      "        [13, 37, 53]])\n"
     ]
    }
   ],
   "source": [
    "# both arguments 2D\n",
    "mat_1 = torch.tensor([[1, 2, 3],\n",
    "                      [4, 3, 8],\n",
    "                      [1, 7, 2]])\n",
    "  \n",
    "mat_2 = torch.tensor([[2, 4, 1],\n",
    "                      [1, 3, 6],\n",
    "                      [2, 6, 5]])\n",
    "  \n",
    "out = torch.matmul(mat_1, mat_2)\n",
    "  \n",
    "print(\"\\n3x3 dimensional tensors :\\n\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "84073eab",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1D-2D multiplication :\n",
      " tensor([29, 38, 61])\n"
     ]
    }
   ],
   "source": [
    "# Arguments of different dimensions\n",
    "# first argument 1D and second argument 2D\n",
    "mat1_1 = torch.tensor([3, 6, 2])\n",
    "  \n",
    "mat1_2 = torch.tensor([[1, 2, 3],\n",
    "                       [4, 3, 8],\n",
    "                       [1, 7, 2]])\n",
    "  \n",
    "out_1 = torch.matmul(mat1_1, mat1_2)\n",
    "print(\"\\n1D-2D multiplication :\\n\", out_1)\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd76b5bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2D-1D multiplication :\n",
      " tensor([21, 61, 59])\n"
     ]
    }
   ],
   "source": [
    "# first argument 2D and second argument 1D\n",
    "mat2_1 = torch.tensor([[2, 4, 1],\n",
    "                       [1, 3, 6],\n",
    "                       [2, 6, 5]])\n",
    "  \n",
    "mat2_2 = torch.tensor([4, 1, 9])\n",
    "  \n",
    "# assigning to output tensor\n",
    "out_2 = torch.matmul(mat2_1, mat2_2)\n",
    "  \n",
    "print(\"\\n2D-1D multiplication :\\n\", out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0f2d0f3",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix A :\n",
      " tensor([[[ 1.9742, -0.3101,  0.9207],\n",
      "         [-0.5845,  0.3964,  0.2774],\n",
      "         [ 0.1423, -0.6356, -1.0601]],\n",
      "\n",
      "        [[ 0.7488,  0.0633, -0.1190],\n",
      "         [ 0.2655, -1.8284, -1.4647],\n",
      "         [ 0.2355, -0.5253,  0.6223]]])\n",
      "\n",
      "matrix B :\n",
      " tensor([ 0.7186, -1.4878,  1.3330])\n",
      "\n",
      "Output :\n",
      " tensor([[ 3.1072, -0.6399, -0.3651],\n",
      "        [ 0.2852,  0.9588,  1.7803]])\n"
     ]
    }
   ],
   "source": [
    "# N-dimensional argument (N>2)\n",
    "# creating Tensors using randn()\n",
    "mat_1 = torch.randn(2, 3, 3)\n",
    "mat_2 = torch.randn(3)\n",
    "  \n",
    "# printing the matrices\n",
    "print(\"matrix A :\\n\", mat_1)\n",
    "print(\"\\nmatrix B :\\n\", mat_2)\n",
    "  \n",
    "# output\n",
    "print(\"\\nOutput :\\n\", torch.matmul(mat_1, mat_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c794a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## That's it for today\n",
    "\n",
    "Next time we will discuss more advanced linear algebra topics before moving onto auto-differentiation!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
